Skienna:

Algorithms for optimization problems require proof that they always return
the best possible solution. Greedy algorithms that make the best local decision
at each step are typically efficient but usually do not guarantee global optimality.
Exhaustive search algorithms that try all possibilities and select the best always
produce the optimum result, but usually at a prohibitive cost in terms of time
complexity.

Dynamic programming combines the best of both worlds. It gives us a way to
design custom algorithms that systematically search all possibilities (thus guaranteeing correctness)
while storing results to avoid recomputing (thus providing
efficiency). By storing the consequences of all possible decisions and using this
information in a systematic way, the total amount of work is minimized.

Once you understand it, dynamic programming is probably the easiest algorithm design
technique to apply in practice. In fact, I find that dynamic programming algorithms
are often easier to reinvent than to try to look up in a book. That
said, until you understand dynamic programming, it seems like magic. You must
figure out the trick before you can use it.

Dynamic programming is a technique for efficiently implementing a recursive
algorithm by storing partial results. The trick is seeing whether the naive recursive
algorithm computes the same subproblems over and over and over again. If so,
storing the answer for each subproblems in a table to look up instead of recompute
can lead to an efficient algorithm. Start with a recursive algorithm or definition.
Only once we have a correct recursive algorithm do we worry about speeding it up
by using a results matrix.

Dynamic programming is generally the right method for optimization problems
on combinatorial objects that have an inherent left to right order among components.
Left-to-right objects includes: character strings, rooted trees, polygons, and
integer sequences. Dynamic programming is best learned by carefully studying
examples until things start to click.

This general method of explicitly caching results from recursive calls to avoid
recomputation provides a simple way to get most of the benefits of full dynamic
programming, so it is worth a more careful look.In principle, such caching can be
employed on any recursive algorithm. However, storing partial results would have
done absolutely no good for such recursive algorithms as quicksort, backtracking,
and depth-first search because all the recursive calls made in these algorithms have
distinct parameter values. It doesnâ€™t pay to store something you will never refer to
again.

Take-Home Lesson: Explicit caching of the results of recursive calls provides
most of the benefits of dynamic programming, including usually the same
running time as the more elegant full solution.
