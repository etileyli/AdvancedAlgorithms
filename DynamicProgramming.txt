Skienna:

Algorithms for optimization problems require proof that they always return
the best possible solution. Greedy algorithms that make the best local decision
at each step are typically efficient but usually do not guarantee global optimality.
Exhaustive search algorithms that try all possibilities and select the best always
produce the optimum result, but usually at a prohibitive cost in terms of time
complexity.

Dynamic programming combines the best of both worlds. It gives us a way to
design custom algorithms that systematically search all possibilities (thus guaranteeing correctness)
while storing results to avoid recomputing (thus providing
efficiency). By storing the consequences of all possible decisions and using this
information in a systematic way, the total amount of work is minimized.

Once you understand it, dynamic programming is probably the easiest algorithm design
technique to apply in practice. In fact, I find that dynamic programming algorithms
are often easier to reinvent than to try to look up in a book. That
said, until you understand dynamic programming, it seems like magic. You must
figure out the trick before you can use it.

Dynamic programming is a technique for efficiently implementing a recursive
algorithm by storing partial results. The trick is seeing whether the naive recursive
algorithm computes the same subproblems over and over and over again. If so,
storing the answer for each subproblems in a table to look up instead of recompute
can lead to an efficient algorithm. Start with a recursive algorithm or definition.
Only once we have a correct recursive algorithm do we worry about speeding it up
by using a results matrix.

Dynamic programming is generally the right method for optimization problems
on combinatorial objects that have an inherent left to right order among components.
Left-to-right objects includes: character strings, rooted trees, polygons, and
integer sequences. Dynamic programming is best learned by carefully studying
examples until things start to click. 
